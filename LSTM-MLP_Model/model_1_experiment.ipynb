{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    number of trajectories in each city\\n    # austin --  train: 43041 test: 6325 \\n    # miami -- train: 55029 test:7971\\n    # pittsburgh -- train: 43544 test: 6361\\n    # dearborn -- train: 24465 test: 3671\\n    # washington-dc -- train: 25744 test: 3829\\n    # palo-alto -- train:  11993 test:1686\\n\\n    trajectories sampled at 10HZ rate, input 5 seconds, output 6 seconds\\n    \\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os, os.path \n",
    "import numpy \n",
    "import pickle\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"\n",
    "    number of trajectories in each city\n",
    "    # austin --  train: 43041 test: 6325 \n",
    "    # miami -- train: 55029 test:7971\n",
    "    # pittsburgh -- train: 43544 test: 6361\n",
    "    # dearborn -- train: 24465 test: 3671\n",
    "    # washington-dc -- train: 25744 test: 3829\n",
    "    # palo-alto -- train:  11993 test:1686\n",
    "\n",
    "    trajectories sampled at 10HZ rate, input 5 seconds, output 6 seconds\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of training data:203816\n",
      "the number of training data:29843\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "ROOT_PATH = \"\"\n",
    "\n",
    "cities = [\"austin\", \"miami\", \"pittsburgh\", \"dearborn\", \"washington-dc\", \"palo-alto\"]\n",
    "splits = [\"train\", \"test\"]\n",
    "\n",
    "def get_city_trajectories(city=\"palo-alto\", split=\"train\", normalized=False):\n",
    "    f_in = ROOT_PATH + split + \"/\" + city + \"_inputs\"\n",
    "    inputs = pickle.load(open(f_in, \"rb\"))\n",
    "    inputs = np.asarray(inputs)\n",
    "    \n",
    "    outputs = None\n",
    "    \n",
    "    if split==\"train\":\n",
    "        f_out = ROOT_PATH + split + \"/\" + city + \"_outputs\"\n",
    "        outputs = pickle.load(open(f_out, \"rb\"))\n",
    "        outputs = np.asarray(outputs)\n",
    "\n",
    "    return inputs, outputs\n",
    "\n",
    "class ArgoverseDataset(Dataset):\n",
    "    \"\"\"Dataset class for Argoverse\"\"\"\n",
    "    def __init__(self, city: str, split:str, transform=None):\n",
    "        super(ArgoverseDataset, self).__init__()\n",
    "        self.transform = transform\n",
    "\n",
    "        self.inputs, self.outputs = get_city_trajectories(city=city, split=split, normalized=False)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        data = (self.inputs[idx], self.outputs[idx])\n",
    "            \n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return data\n",
    "\n",
    "# intialize a dataset\n",
    "count_train = 0\n",
    "for i in cities:\n",
    "    city = i  \n",
    "    split = 'train'\n",
    "    train_dataset  = ArgoverseDataset(city = city, split = split)\n",
    "    count_train += len(train_dataset)\n",
    "print(\"the number of training data:\" + str(count_train))\n",
    "\n",
    "count_test = 0\n",
    "for i in cities:\n",
    "    city = i  \n",
    "    split = 'test'\n",
    "    test_dataset  = ArgoverseDataset(city = city, split = split)\n",
    "    count_test+= len(test_dataset)\n",
    "print(\"the number of training data:\" + str(count_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the original training document into trainable batches \n",
    "def get_batches_x(inp, batch_size):\n",
    "    \n",
    "    \n",
    "    num_batches = int(len(inp) / (batch_size))\n",
    "    \n",
    "    X = inp[:num_batches*batch_size]\n",
    "#     Y = np.zeros((len(X), 50))\n",
    "    \n",
    "    X = X[:, :, 0]\n",
    "    \n",
    "    # generate trainable batches\n",
    "    for i in range(0, num_batches*batch_size, batch_size):\n",
    "        Xs = []\n",
    "        Ys = []\n",
    "        for j in range(i, i+batch_size):\n",
    "            temp_x = X[j][:-1]\n",
    "               \n",
    "            temp_y = X[j][1:]\n",
    "            \n",
    "            Xs.append(temp_x)\n",
    "            Ys.append(temp_y)\n",
    "        \n",
    "        Xs = np.array(Xs)\n",
    "        Ys = np.array(Ys)\n",
    "\n",
    "        yield torch.tensor(Xs).float(), torch.tensor(Ys).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RNN_new_1(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        super(RNN_new_1, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.encode1 = nn.Linear(input_size, hidden_size)\n",
    "        # input tensor - (batch, seq, tensor)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers=n_layers, batch_first=True)\n",
    "        self.bilstm = nn.LSTM(hidden_size, hidden_size//2, num_layers=n_layers, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.encode2 = nn.Linear(input_size, hidden_size)\n",
    "        \n",
    "        self.encode3 = nn.Linear(hidden_size, hidden_size * 2)\n",
    "        \n",
    "        self.encode4 = nn.Linear(hidden_size * 2, hidden_size // 2)\n",
    "        \n",
    "        self.encode5 = nn.Linear(input_size, hidden_size // 2)\n",
    "        \n",
    "        self.encode6 = nn.Linear(hidden_size//2, output_size)\n",
    "        \n",
    "        \n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "#         self.encode1 = nn.Embedding(61, (hidden_size-10)//2)\n",
    "#         self.encode2 = nn.Embedding(max_frequency+1, (hidden_size-10)//2)\n",
    "#         self.encode3 = nn.Embedding(5, 10)\n",
    "        \n",
    "\n",
    "    def forward(self, x_input, prev_hidden_and_cell, temperature=1.0):\n",
    "        \n",
    "        encode1 = self.encode1(x_input)\n",
    "\n",
    "        output, (hidden, cell) = self.lstm(encode1.unsqueeze(1), prev_hidden_and_cell)\n",
    "#         print(output.shape)\n",
    "        encode2 = self.encode2(x_input)\n",
    "        bi_output, (hidden, cell) = self.lstm(encode2.unsqueeze(1), prev_hidden_and_cell)\n",
    "#         print(bi_output.shape)\n",
    "        f_output = torch.tanh(self.encode3(output + bi_output))\n",
    "#         print(f_output.shape)\n",
    "        f_output1 = torch.tanh(self.encode4(f_output))\n",
    "#         print(f_output1.shape)\n",
    "        f_output2 = self.encode5(x_input)\n",
    "#         print(f_output2.shape)\n",
    "#         print(f_output1.shape)\n",
    "#         print((f_output1 + f_output2).shape)\n",
    "        \n",
    "        f_output2 = f_output2.reshape(f_output2.shape[0], 1, f_output2.shape[1])\n",
    "#         print(f_output2.shape)\n",
    "#         print((f_output1 + f_output2).shape)\n",
    "#         print(f_output1.shape)\n",
    "        output = self.encode6(f_output1 + f_output2)\n",
    "        output = output.reshape(len(output), 1)\n",
    "#         print(output)\n",
    "#         print(output)\n",
    "#         print(output.shape)\n",
    "#         output = self.decoder(output.reshape(output.shape[0], -1))\n",
    "\n",
    "#         print(output)\n",
    "#         print(output)\n",
    "#         print(output.shape)\n",
    "#         output = self.softmax(output / temperature)\n",
    "#         output = defined_softmax(output.reshape(1, 1, 61), temperature)\n",
    "\n",
    "        return output, (hidden, cell)\n",
    "    \n",
    "    def forward1(self, input_char_idx, input_char_freq, input_char_syllable, prev_hidden_and_cell, temperature=1.0):\n",
    "\n",
    "        encode1 = self.encode1(input_char_idx)\n",
    "        encode2 = self.encode2(input_char_freq)\n",
    "        encode3 = self.encode3(input_char_syllable)\n",
    "        \n",
    "        encode = torch.cat((encode1, encode2, encode3), 1)\n",
    "\n",
    "        output, (hidden, cell) = self.lstm(encode.unsqueeze(1), prev_hidden_and_cell)\n",
    "\n",
    "        output = self.decoder(output.reshape(output.shape[0], -1))\n",
    "\n",
    "        output = self.softmax(output / temperature)\n",
    "#         output = defined_softmax(output, temperature)\n",
    "\n",
    "\n",
    "        return output, (hidden, cell)\n",
    "\n",
    "    def init_state(self, batch_size):\n",
    "        # return the initial hidden state and cell state\n",
    "        return torch.zeros(self.input_size, batch_size, self.hidden_size), torch.zeros(self.input_size, batch_size, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(inp, n_epochs, hidden_size, batch_size, seq_size, lr, temperature):\n",
    "    \n",
    "    # (input_size/feature dim, num of characters, hidden_size, output_size, n_layers=1)\n",
    "    model = RNN_new_1(1, hidden_size, 1, 1)\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    avg_loss_over_time = []\n",
    "    for epoch in range(n_epochs):\n",
    "        batches = get_batches_x(inp, batch_size)\n",
    "\n",
    "        hidden, cell = model.init_state(batch_size)\n",
    "\n",
    "        i = 0\n",
    "        print(\"new epoch starts, currently at \" + str(epoch) + \" epoch\")\n",
    "        \n",
    "        loss_avg = 0\n",
    "        \n",
    "        for X, Y in batches:\n",
    "            i += 1\n",
    "\n",
    "            # Reset all gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss = 0\n",
    "                         \n",
    "            for i in range(seq_size-1):\n",
    "                temp_x = X[:, i]\n",
    "                temp_y = Y[:, i]\n",
    "                output, (hidden, cell) = model.forward(temp_x.reshape(batch_size, 1), (hidden, cell), temperature)\n",
    "#                 print(output)\n",
    "#                 print(output.shape)\n",
    "                loss += criterion(output, temp_y.reshape(batch_size, 1))\n",
    "            \n",
    "            hidden = hidden.detach()\n",
    "            cell = cell.detach()\n",
    "            \n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            loss_avg += loss.item() / seq_size\n",
    "            \n",
    "        avg_loss_over_time.append(math.sqrt(loss_avg / i))   \n",
    "        print(\"avg loss at \" + str(i) + \"th iteration - \" + str(math.sqrt(loss_avg / i)))\n",
    "        print(\"\\n\")  \n",
    "    \n",
    "    \n",
    "    plt.plot(avg_loss_over_time)\n",
    "    plt.xlabel(\"epoch num\")\n",
    "    plt.ylabel(\"avg_loss\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_i, p_o = get_city_trajectories(city=\"palo-alto\", split=\"train\", normalized=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_combined = np.concatenate([p_i, p_o], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11993, 110, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_mean, temp_std = np.mean(train_combined[:, :, 0].reshape(-1)), np.std(train_combined[:, :, 0].reshape(-1))\n",
    "normalized = (train_combined[:, :, 0].reshape(-1) - temp_mean) / temp_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starts training process\n",
      "new epoch starts, currently at 0 epoch\n",
      "avg loss at 108th iteration - 269.03168453555486\n",
      "\n",
      "\n",
      "new epoch starts, currently at 1 epoch\n",
      "avg loss at 108th iteration - 55.332928648415056\n",
      "\n",
      "\n",
      "new epoch starts, currently at 2 epoch\n",
      "avg loss at 108th iteration - 16.369735432711952\n",
      "\n",
      "\n",
      "new epoch starts, currently at 3 epoch\n",
      "avg loss at 108th iteration - 5.7912167561985095\n",
      "\n",
      "\n",
      "new epoch starts, currently at 4 epoch\n",
      "avg loss at 108th iteration - 1.7501580651822426\n",
      "\n",
      "\n",
      "new epoch starts, currently at 5 epoch\n",
      "avg loss at 108th iteration - 0.668172156090779\n",
      "\n",
      "\n",
      "new epoch starts, currently at 6 epoch\n",
      "avg loss at 108th iteration - 0.42926282995899023\n",
      "\n",
      "\n",
      "new epoch starts, currently at 7 epoch\n",
      "avg loss at 108th iteration - 0.40180785835295446\n",
      "\n",
      "\n",
      "new epoch starts, currently at 8 epoch\n",
      "avg loss at 108th iteration - 0.3930175955172718\n",
      "\n",
      "\n",
      "new epoch starts, currently at 9 epoch\n",
      "avg loss at 108th iteration - 0.38746986126940586\n",
      "\n",
      "\n",
      "Ends training process, total time - 683.1844065189362\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# (inp, n_epochs, hidden_size, batch_size, seq_size, lr, temperature)\n",
    "start = time.time()\n",
    "print(\"Starts training process\")\n",
    "model_1 = train_model(train_combined, 10, 64, 500, 110, 0.05, 1.5)\n",
    "end = time.time()\n",
    "print(\"Ends training process, total time - \" + str(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, inp, predict_length):\n",
    "\n",
    "    hidden, cell = model.init_state(1)\n",
    "    \n",
    "    \n",
    "    input_batch = get_batches_x(inp.reshape(1,len(inp),2), 1)\n",
    "\n",
    "    X, _ = next(input_batch)\n",
    "    \n",
    "    # build upon our hidden and cell state based on the first seq_size-1 positions\n",
    "    for i in range(len(inp)-1):\n",
    "        temp_x = X[:, i]\n",
    "#         print(temp_x)\n",
    "        output, (hidden, cell) = model.forward(temp_x.reshape(1, 1), (hidden, cell), 1)\n",
    "#         print(output)\n",
    "\n",
    "    prev = inp[-1][0]\n",
    "#     print(prev)\n",
    "    predict = []\n",
    "    # make the prediction character by character\n",
    "    for _ in range(predict_length):\n",
    "        \n",
    "        temp_x = torch.tensor([prev]).float()\n",
    "\n",
    "        output, (hidden, cell) = model.forward(temp_x.reshape(1,1), (hidden, cell), 1)\n",
    "\n",
    "#         output = defined_softmax(output.reshape(1, 1, 61), temperature)\n",
    "\n",
    "        prev = output[0]\n",
    "\n",
    "        predict.append(prev)\n",
    "\n",
    "\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
