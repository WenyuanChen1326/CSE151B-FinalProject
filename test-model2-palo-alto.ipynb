{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-21T21:38:14.110047Z","iopub.execute_input":"2022-05-21T21:38:14.112427Z","iopub.status.idle":"2022-05-21T21:38:14.160705Z","shell.execute_reply.started":"2022-05-21T21:38:14.112294Z","shell.execute_reply":"2022-05-21T21:38:14.159684Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip3 install pickle5","metadata":{"execution":{"iopub.status.busy":"2022-05-21T00:56:57.789871Z","iopub.execute_input":"2022-05-21T00:56:57.790407Z","iopub.status.idle":"2022-05-21T00:57:09.067531Z","shell.execute_reply.started":"2022-05-21T00:56:57.790358Z","shell.execute_reply":"2022-05-21T00:57:09.06671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nimport os, os.path \nimport numpy \nimport pickle\nfrom glob import glob\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2022-05-21T21:38:14.161883Z","iopub.execute_input":"2022-05-21T21:38:14.162329Z","iopub.status.idle":"2022-05-21T21:38:16.880332Z","shell.execute_reply.started":"2022-05-21T21:38:14.162296Z","shell.execute_reply":"2022-05-21T21:38:16.879321Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"pip install pickle5","metadata":{"execution":{"iopub.status.busy":"2022-05-21T21:38:49.432765Z","iopub.execute_input":"2022-05-21T21:38:49.433461Z","iopub.status.idle":"2022-05-21T21:39:04.231528Z","shell.execute_reply.started":"2022-05-21T21:38:49.433414Z","shell.execute_reply":"2022-05-21T21:39:04.230379Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from glob import glob\nimport pickle5 as pickle\nimport numpy as np\n\nROOT_PATH = \"../input/\"\n\ncities = [\"austin\", \"miami\", \"pittsburgh\", \"dearborn\", \"washington-dc\", \"palo-alto\"]\nsplits = [\"train\", \"test\"]\n\ndef get_city_trajectories(city=\"palo-alto\", split=\"train\", normalized=False):\n    if split==\"train\":\n        f_in = ROOT_PATH + split + \"/\" + city + \"_inputs\"\n        inputs = pickle.load(open(f_in, \"rb\"))\n        inputs = np.asarray(inputs)\n\n        outputs = None\n    \n        f_out = ROOT_PATH + split + \"/\" + city + \"_outputs\"\n        outputs = pickle.load(open(f_out, \"rb\"))\n        outputs = np.asarray(outputs)\n        \n    elif split==\"test\":\n        f_in = ROOT_PATH + \"test-inputs\" + \"/\" + city + \"_inputs\"\n        inputs = pickle.load(open(f_in, \"rb\"))\n        inputs = np.asarray(inputs)\n\n        outputs = None\n        \n    return inputs, outputs\n\n# class ArgoverseDataset(Dataset):\n#     \"\"\"Dataset class for Argoverse\"\"\"\n#     def __init__(self, city: str, split:str, transform=None):\n#         super(ArgoverseDataset, self).__init__()\n#         self.transform = transform\n\n#         self.inputs, self.outputs = get_city_trajectories(city=city, split=split, normalized=False)\n\n#     def __len__(self):\n#         return len(self.inputs)\n\n#     def __getitem__(self, idx):\n\n#         data = (self.inputs[idx], self.outputs[idx])\n            \n#         if self.transform:\n#             data = self.transform(data)\n\n#         return data\n\n# # intialize a dataset\n# count_train = 0\n# for i in cities:\n#     city = i  \n#     split = 'train'\n#     train_dataset  = ArgoverseDataset(city = city, split = split)\n#     count_train += len(train_dataset)\n# print(\"the number of training data:\" + str(count_train))\n\n# count_test = 0\n# for i in cities:\n#     city = i  \n#     split = 'test'\n#     test_dataset  = ArgoverseDataset(city = city, split = split)\n#     count_test+= len(test_dataset)\n# print(\"the number of training data:\" + str(count_test))","metadata":{"execution":{"iopub.status.busy":"2022-05-21T21:39:10.148847Z","iopub.execute_input":"2022-05-21T21:39:10.149844Z","iopub.status.idle":"2022-05-21T21:39:10.170266Z","shell.execute_reply.started":"2022-05-21T21:39:10.149780Z","shell.execute_reply":"2022-05-21T21:39:10.169173Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# convert the original training document into trainable batches \ndef get_batches(inp, batch_size):\n    \n    num_batches = int(len(inp) / (batch_size))\n    \n    X = inp[:num_batches*batch_size]\n#     Y = np.zeros((len(X), 50))\n    \n    \n    # generate trainable batches\n    for i in range(0, num_batches*batch_size, batch_size):\n        Xs = []\n        Ys = []\n        for j in range(i, i+batch_size):\n            temp_x = X[j][:-1]\n               \n            temp_y = X[j][1:]\n            \n            Xs.append(temp_x)\n            Ys.append(temp_y)\n        \n        Xs = np.array(Xs)\n        Ys = np.array(Ys)\n\n        yield torch.tensor(Xs).float(), torch.tensor(Ys).float()","metadata":{"execution":{"iopub.status.busy":"2022-05-21T21:39:11.017496Z","iopub.execute_input":"2022-05-21T21:39:11.017876Z","iopub.status.idle":"2022-05-21T21:39:11.027099Z","shell.execute_reply.started":"2022-05-21T21:39:11.017838Z","shell.execute_reply":"2022-05-21T21:39:11.026031Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\n\nclass RNN_new_1(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size, n_layers=2):\n        super(RNN_new_1, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.n_layers = n_layers\n        self.encode1 = nn.Linear(input_size, hidden_size)\n        # input tensor - (batch, seq, tensor)\n        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers=n_layers, batch_first=True)\n        self.bilstm = nn.LSTM(hidden_size, hidden_size//2, num_layers=n_layers, batch_first=True, bidirectional=True)\n        \n        self.encode2 = nn.Linear(input_size, hidden_size)\n        \n        self.encode3 = nn.Linear(hidden_size, hidden_size * 2)\n        \n        self.encode4 = nn.Linear(hidden_size * 2, hidden_size // 2)\n        \n        self.encode5 = nn.Linear(input_size, hidden_size // 2)\n        \n        self.encode6 = nn.Linear(hidden_size//2, output_size)\n        \n        \n        self.decoder = nn.Linear(hidden_size, output_size)\n        \n\n        \n    def forward(self, x_input, prev_hidden_and_cell, temperature=1.0):\n        \n        encode1 = self.encode1(x_input)\n\n#         print(encode1.unsqueeze(1).shape)\n        output, (hidden, cell) = self.lstm(encode1.unsqueeze(1), prev_hidden_and_cell)\n        \n        encode2 = self.encode2(x_input)\n        bi_output, (hidden, cell) = self.lstm(encode2.unsqueeze(1), prev_hidden_and_cell)\n#         print(bi_output.shape)\n        f_output = torch.tanh(self.encode3(output + bi_output))\n#         print(f_output.shape)\n        f_output1 = torch.tanh(self.encode4(f_output))\n#         print(f_output1.shape)\n        f_output2 = self.encode5(x_input)\n        \n        f_output2 = f_output2.reshape(f_output2.shape[0], 1, f_output2.shape[1])\n\n        output = self.encode6(f_output1 + f_output2)\n        output = output.reshape(len(output), 2)\n#         print(output.shape)\n\n        return output, (hidden, cell)\n\n\n    def init_state(self, batch_size):\n        # return the initial hidden state and cell state\n        return torch.zeros(self.n_layers, batch_size, self.hidden_size), torch.zeros(self.n_layers, batch_size, self.hidden_size)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T21:39:12.150064Z","iopub.execute_input":"2022-05-21T21:39:12.150452Z","iopub.status.idle":"2022-05-21T21:39:12.168221Z","shell.execute_reply.started":"2022-05-21T21:39:12.150416Z","shell.execute_reply":"2022-05-21T21:39:12.167181Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def train_model(inp, n_epochs, hidden_size, batch_size, seq_size, lr, temperature):\n    \n    # (input_size/feature dim, num of characters, hidden_size, output_size, n_layers=1)\n    model = RNN_new_1(2, hidden_size, 2, 1)\n    model.train()\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    criterion = nn.MSELoss()\n    avg_loss_over_time = []\n    for epoch in range(n_epochs):\n        batches = get_batches(inp, batch_size)\n\n        hidden, cell = model.init_state(batch_size)\n\n        i = 0\n        print(\"new epoch starts, currently at \" + str(epoch) + \" epoch\")\n        \n        loss_avg = 0\n        \n        for X, Y in batches:\n            i += 1\n\n            # Reset all gradients\n            optimizer.zero_grad()\n            \n            loss = 0\n                         \n            for i in range(seq_size-1):\n                temp_x = X[:, i]\n                temp_y = Y[:, i]\n                output, (hidden, cell) = model.forward(temp_x.reshape(batch_size, 2), (hidden, cell), temperature)\n#                 print(output)\n#                 print(output.shape)\n                loss += criterion(output, temp_y.reshape(batch_size, 2))\n            \n            hidden = hidden.detach()\n            cell = cell.detach()\n            \n            loss.backward(retain_graph=True)\n            optimizer.step()\n            loss_avg += loss.item() / seq_size\n            \n        avg_loss_over_time.append(loss_avg / i)\n        print(\"avg loss at \" + str(i) + \"th iteration - \" + str(loss_avg / i))\n        print(\"\\n\")  \n    \n    \n    plt.plot(avg_loss_over_time)\n    plt.xlabel(\"epoch num\")\n    plt.ylabel(\"avg_loss\")\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-05-21T21:39:12.964187Z","iopub.execute_input":"2022-05-21T21:39:12.964544Z","iopub.status.idle":"2022-05-21T21:39:12.977389Z","shell.execute_reply.started":"2022-05-21T21:39:12.964509Z","shell.execute_reply":"2022-05-21T21:39:12.976437Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nimport math","metadata":{"execution":{"iopub.status.busy":"2022-05-21T21:39:14.097133Z","iopub.execute_input":"2022-05-21T21:39:14.097982Z","iopub.status.idle":"2022-05-21T21:39:14.102073Z","shell.execute_reply.started":"2022-05-21T21:39:14.097929Z","shell.execute_reply":"2022-05-21T21:39:14.101272Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"p_i, p_o = get_city_trajectories(city=\"palo-alto\", split=\"train\", normalized=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T21:39:14.890107Z","iopub.execute_input":"2022-05-21T21:39:14.890489Z","iopub.status.idle":"2022-05-21T21:39:15.286055Z","shell.execute_reply.started":"2022-05-21T21:39:14.890451Z","shell.execute_reply":"2022-05-21T21:39:15.285397Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"train_combined = np.concatenate([p_i, p_o], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T21:39:15.727083Z","iopub.execute_input":"2022-05-21T21:39:15.727700Z","iopub.status.idle":"2022-05-21T21:39:15.744739Z","shell.execute_reply.started":"2022-05-21T21:39:15.727661Z","shell.execute_reply":"2022-05-21T21:39:15.743789Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"p_test_i, p_test_o = get_city_trajectories(city=\"palo-alto\", split=\"test\", normalized=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T21:39:16.744517Z","iopub.execute_input":"2022-05-21T21:39:16.744905Z","iopub.status.idle":"2022-05-21T21:39:16.809736Z","shell.execute_reply.started":"2022-05-21T21:39:16.744861Z","shell.execute_reply":"2022-05-21T21:39:16.808909Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def normalize(train_combined):\n    \n    x_mean = np.mean(train_combined[:, :, 0].reshape(-1))\n    x_std = np.std(train_combined[:, :, 0].reshape(-1))\n    x_norm = ((train_combined[:, :, 0].reshape(-1) - x_mean) / x_std).reshape(train_combined.shape[0], train_combined.shape[1], 1)\n\n    y_mean = np.mean(train_combined[:, :, 1].reshape(-1))\n    y_std = np.std(train_combined[:, :, 1].reshape(-1))\n    y_norm = ((train_combined[:, :, 1].reshape(-1) - y_mean) / y_std).reshape(train_combined.shape[0], train_combined.shape[1], 1)\n\n    normalized = np.concatenate([x_norm, y_norm], axis=2)\n    \n    return normalized","metadata":{"execution":{"iopub.status.busy":"2022-05-21T21:39:17.801005Z","iopub.execute_input":"2022-05-21T21:39:17.801663Z","iopub.status.idle":"2022-05-21T21:39:17.811076Z","shell.execute_reply.started":"2022-05-21T21:39:17.801612Z","shell.execute_reply":"2022-05-21T21:39:17.810159Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"train_combined_norm = normalize(train_combined)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T21:39:18.908927Z","iopub.execute_input":"2022-05-21T21:39:18.910336Z","iopub.status.idle":"2022-05-21T21:39:18.991294Z","shell.execute_reply.started":"2022-05-21T21:39:18.910280Z","shell.execute_reply":"2022-05-21T21:39:18.990122Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# (inp, n_epochs, hidden_size, batch_size, seq_size, lr, temperature)\nstart = time.time()\nprint(\"Starts training process\")\nmodel_1 = train_model(train_combined, 20, 128, 500, 110, 0.05, 1.5)\nend = time.time()\nprint(\"Ends training process, total time - \" + str(end - start))","metadata":{"execution":{"iopub.status.busy":"2022-05-21T21:39:24.445793Z","iopub.execute_input":"2022-05-21T21:39:24.446282Z","iopub.status.idle":"2022-05-21T21:52:23.499783Z","shell.execute_reply.started":"2022-05-21T21:39:24.446239Z","shell.execute_reply":"2022-05-21T21:52:23.498753Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"x_global_mean = np.mean(train_combined[:, :, 0].reshape(-1))\nx_global_std = np.std(train_combined[:, :, 0].reshape(-1))\ny_global_mean = np.mean(train_combined[:, :, 1].reshape(-1))\ny_global_std = np.std(train_combined[:, :, 1].reshape(-1))","metadata":{"execution":{"iopub.status.busy":"2022-05-21T02:06:46.745241Z","iopub.execute_input":"2022-05-21T02:06:46.745723Z","iopub.status.idle":"2022-05-21T02:06:46.76994Z","shell.execute_reply.started":"2022-05-21T02:06:46.745689Z","shell.execute_reply":"2022-05-21T02:06:46.768982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(model, inp, predict_length, normalize_test_data=False):\n    \n    if normalize_test_data:\n        x_mean = np.mean(inp[:, 0].reshape(-1))\n        x_std = np.std(inp[:, 0].reshape(-1))\n        y_mean = np.mean(inp[:, 1].reshape(-1))\n        y_std = np.std(inp[:, 1].reshape(-1))\n        x_norm = ((inp[:, 0].reshape(-1) - x_mean) / x_std).reshape(inp.shape[0], 1)\n        y_norm = ((inp[:, 1].reshape(-1) - y_mean) / y_std).reshape(inp.shape[0], 1)\n\n        norm = np.concatenate([x_norm, y_norm], axis=1)\n    \n        inp = norm\n\n    \n    hidden, cell = model.init_state(1)\n    \n    \n    input_batch = get_batches(inp.reshape(1,len(inp),2), 1)\n\n    X, _ = next(input_batch)\n    \n    # build upon our hidden and cell state based on the first seq_size-1 positions\n    for i in range(len(inp)-1):\n        temp_x = X[:, i]\n#         print(temp_x)\n        output, (hidden, cell) = model.forward(temp_x, (hidden, cell))\n\n#         print(output)\n\n    prev = inp[-1]\n#     print(prev)\n    predict = []\n    # make the prediction character by character\n    for _ in range(predict_length):\n        \n        if _ == 0:\n            temp_x = torch.tensor([prev]).float()\n        else:\n            temp_x = prev\n       \n        output, (hidden, cell) = model.forward(temp_x, (hidden, cell))\n\n#         output = defined_softmax(output.reshape(1, 1, 61), temperature)\n\n        prev = output\n        \n        if normalize_test_data:\n            x_pred = (prev[0][0].item()) * x_std + x_mean\n            y_pred = (prev[0][1].item()) * y_std + y_mean\n        else:\n            x_pred = (prev[0][0].item())\n            y_pred = (prev[0][1].item())\n            \n        predict.append([x_pred, y_pred])\n    \n    return predict","metadata":{"execution":{"iopub.status.busy":"2022-05-21T02:19:49.322831Z","iopub.execute_input":"2022-05-21T02:19:49.323099Z","iopub.status.idle":"2022-05-21T02:19:49.33624Z","shell.execute_reply.started":"2022-05-21T02:19:49.323071Z","shell.execute_reply":"2022-05-21T02:19:49.335342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p_o[0]","metadata":{"execution":{"iopub.status.busy":"2022-05-21T02:20:07.19977Z","iopub.execute_input":"2022-05-21T02:20:07.200065Z","iopub.status.idle":"2022-05-21T02:20:07.207931Z","shell.execute_reply.started":"2022-05-21T02:20:07.200032Z","shell.execute_reply":"2022-05-21T02:20:07.207088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict(model_1, p_i[0], 60, False)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T02:20:45.488298Z","iopub.execute_input":"2022-05-21T02:20:45.489179Z","iopub.status.idle":"2022-05-21T02:20:45.565319Z","shell.execute_reply.started":"2022-05-21T02:20:45.489127Z","shell.execute_reply":"2022-05-21T02:20:45.564416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"palo_df = pd.DataFrame()\nfor i in range(len(p_test_i)):\n    \n    pred = predict(model_1, p_test_i[i], 60, False)\n    print(\"predicts for \" + str(i) + \" is done\")\n    temp_dict = {}\n    for j in range(0, 120, 2):\n        col_x = 'v' + str(j)\n        col_y = 'v' + str(j+1)\n        row = j // 2\n        temp_dict[col_x] = pred[row][0]\n        temp_dict[col_y] = pred[row][1]\n    \n    temp_idx_name = str(i) + \"_\" + \"palo-alto\"\n    temp_final = pd.DataFrame(temp_dict, index=[temp_idx_name])\n    palo_df = pd.concat([palo_df, temp_final])","metadata":{"execution":{"iopub.status.busy":"2022-05-21T02:21:43.457393Z","iopub.execute_input":"2022-05-21T02:21:43.457848Z","iopub.status.idle":"2022-05-21T02:23:42.532652Z","shell.execute_reply.started":"2022-05-21T02:21:43.457799Z","shell.execute_reply":"2022-05-21T02:23:42.531705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"palo_df.reset_index(level=0).rename(columns={\"index\": \"ID\"}).to_csv(\"model2_palo_df.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T02:24:27.845296Z","iopub.execute_input":"2022-05-21T02:24:27.845599Z","iopub.status.idle":"2022-05-21T02:24:28.157526Z","shell.execute_reply.started":"2022-05-21T02:24:27.84557Z","shell.execute_reply":"2022-05-21T02:24:28.156639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_actual_path = \"../input/pred-actual/palo-alto_actual.csv\"\ndf_pred_path = \"../input/pred1/model1_palo_df.csv\"\n\ndef actual_error(df_actual_path, df_pred_path):\n    actual = pd.read_csv(df_actual_path)    \n    actual_array = actual.iloc[:, 1:].to_numpy()\n    \n    pred1 = pd.read_csv(df_pred_path)\n    pred1_array = pred1.iloc[:, 1:].to_numpy()\n    \n    return np.sum((actual_array - pred1_array)**2) / len(actual_array)\n\nactual_error(df_actual_path, df_pred_path)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T02:25:22.846319Z","iopub.execute_input":"2022-05-21T02:25:22.846606Z","iopub.status.idle":"2022-05-21T02:25:22.963019Z","shell.execute_reply.started":"2022-05-21T02:25:22.846577Z","shell.execute_reply":"2022-05-21T02:25:22.962196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n \n # create regressor object\nregressor = RandomForestRegressor(n_estimators = 100, random_state = 0)\n \n","metadata":{"execution":{"iopub.status.busy":"2022-05-21T02:27:57.995417Z","iopub.execute_input":"2022-05-21T02:27:57.995774Z","iopub.status.idle":"2022-05-21T02:27:58.54544Z","shell.execute_reply.started":"2022-05-21T02:27:57.995739Z","shell.execute_reply":"2022-05-21T02:27:58.544599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p_i.shape","metadata":{"execution":{"iopub.status.busy":"2022-05-21T02:33:16.489377Z","iopub.execute_input":"2022-05-21T02:33:16.489694Z","iopub.status.idle":"2022-05-21T02:33:16.49498Z","shell.execute_reply.started":"2022-05-21T02:33:16.489659Z","shell.execute_reply":"2022-05-21T02:33:16.494309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p_i[:, :, 0].shape","metadata":{"execution":{"iopub.status.busy":"2022-05-21T02:34:40.131976Z","iopub.execute_input":"2022-05-21T02:34:40.132307Z","iopub.status.idle":"2022-05-21T02:34:40.138598Z","shell.execute_reply.started":"2022-05-21T02:34:40.132274Z","shell.execute_reply":"2022-05-21T02:34:40.137686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p_o[:, 0, 0].shape","metadata":{"execution":{"iopub.status.busy":"2022-05-21T02:35:26.998765Z","iopub.execute_input":"2022-05-21T02:35:26.999821Z","iopub.status.idle":"2022-05-21T02:35:27.007788Z","shell.execute_reply.started":"2022-05-21T02:35:26.999771Z","shell.execute_reply":"2022-05-21T02:35:27.006431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"regressor.fit(p_i[:, :, 0], p_o[:, 0, 0])","metadata":{"execution":{"iopub.status.busy":"2022-05-21T02:35:38.529762Z","iopub.execute_input":"2022-05-21T02:35:38.530594Z","iopub.status.idle":"2022-05-21T02:36:16.363208Z","shell.execute_reply.started":"2022-05-21T02:35:38.530546Z","shell.execute_reply":"2022-05-21T02:36:16.362193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"regressor.predict(p_i[0,:, 0].reshape(1, -1))[0]","metadata":{"execution":{"iopub.status.busy":"2022-05-21T02:47:46.923936Z","iopub.execute_input":"2022-05-21T02:47:46.924957Z","iopub.status.idle":"2022-05-21T02:47:46.948875Z","shell.execute_reply.started":"2022-05-21T02:47:46.924914Z","shell.execute_reply":"2022-05-21T02:47:46.947828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_data(inp):\n    \n    final_feature = inp[:, :50, 0]\n    final_target = inp[:, 50, 0]\n    for i in range(1, 60):\n        temp_x = inp[:, i:50+i, 0]\n        temp_y = inp[:, 50+i, 0]\n        final_feature = np.concatenate([final_feature, temp_x], axis=0)\n        final_target = np.concatenate([final_target, temp_y], axis=0)\n    \n    return final_feature, final_target\n    ","metadata":{"execution":{"iopub.status.busy":"2022-05-21T03:16:55.048764Z","iopub.execute_input":"2022-05-21T03:16:55.049087Z","iopub.status.idle":"2022-05-21T03:16:55.056148Z","shell.execute_reply.started":"2022-05-21T03:16:55.049057Z","shell.execute_reply":"2022-05-21T03:16:55.055201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X, y = preprocess_data(train_combined)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T03:17:10.736332Z","iopub.execute_input":"2022-05-21T03:17:10.736771Z","iopub.status.idle":"2022-05-21T03:17:13.220881Z","shell.execute_reply.started":"2022-05-21T03:17:10.736738Z","shell.execute_reply":"2022-05-21T03:17:13.220009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"regressor.fit(X, y)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T03:18:17.27498Z","iopub.execute_input":"2022-05-21T03:18:17.275718Z","iopub.status.idle":"2022-05-21T04:04:17.628171Z","shell.execute_reply.started":"2022-05-21T03:18:17.27568Z","shell.execute_reply":"2022-05-21T04:04:17.626562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_regressor(model, inp, pred_len):\n    \n    result = []\n    curr = inp[:, 0]\n    pred = model.predict(curr.reshape(1, -1))[0]\n    result.append(pred)\n    for i in range(1, pred_len):\n        curr = np.append(curr[1:], pred)\n        pred = model.predict(curr.reshape(1, -1))[0]\n        result.append(pred)\n    \n    return result\n    ","metadata":{"execution":{"iopub.status.busy":"2022-05-21T02:52:50.617084Z","iopub.execute_input":"2022-05-21T02:52:50.617728Z","iopub.status.idle":"2022-05-21T02:52:50.624771Z","shell.execute_reply.started":"2022-05-21T02:52:50.617677Z","shell.execute_reply":"2022-05-21T02:52:50.623794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_regressor(regressor, p_i[0], 60)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T02:52:51.084092Z","iopub.execute_input":"2022-05-21T02:52:51.084511Z","iopub.status.idle":"2022-05-21T02:52:51.611075Z","shell.execute_reply.started":"2022-05-21T02:52:51.084481Z","shell.execute_reply":"2022-05-21T02:52:51.61009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_actual_path = \"./austin_actual.csv\"\ndf_pred_path = \"../input/predmodel3/model3_austin.csv\"\n\ndef actual_error(df_actual_path, df_pred_path):\n    actual = pd.read_csv(df_actual_path)    \n    actual_array = actual.iloc[:, 1:].to_numpy()\n    \n    pred1 = pd.read_csv(df_pred_path)\n    pred1_array = pred1.iloc[:, 1:].to_numpy()\n    \n    return np.sum((actual_array - pred1_array)**2) / len(actual_array)\n\nactual_error(df_actual_path, df_pred_path)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T04:09:46.171786Z","iopub.execute_input":"2022-05-21T04:09:46.172046Z","iopub.status.idle":"2022-05-21T04:09:46.622978Z","shell.execute_reply.started":"2022-05-21T04:09:46.172018Z","shell.execute_reply":"2022-05-21T04:09:46.621773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = pd.read_csv(\"../input/predaustin/austin_df_604.csv\")\nb = pd.read_csv(\"../input/predaustin/austin_df_605.csv\")\nc = pd.concat([a, b])\nc.to_csv(\"austin_actual.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T04:09:08.006253Z","iopub.execute_input":"2022-05-21T04:09:08.006544Z","iopub.status.idle":"2022-05-21T04:09:09.366412Z","shell.execute_reply.started":"2022-05-21T04:09:08.006517Z","shell.execute_reply":"2022-05-21T04:09:09.365411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}